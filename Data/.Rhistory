topic = reorder(topic, gamma))
gamma_terms %>%
ggplot(aes(topic, gamma, label = terms, fill = topic)) +
geom_col(show.legend = FALSE) +
geom_text(hjust=0.6,size = 3,nudge_y = 0.15,
family = "IBMPlexSans") +
coord_flip() +
theme_tufte(base_family = "IBMPlexSans", ticks = FALSE) +
theme(plot.title = element_text(size = 16,
family="IBMPlexSans-Bold"),
plot.subtitle = element_text(size = 13)) +
labs(x = NULL, y = expression(gamma),
title = "6 topics by prevalence in the Speech Corpus",
subtitle = "With the top words that contribute to each topic")
top_terms <- td_beta %>%
arrange(beta) %>%
group_by(topic) %>%
top_n(7, beta) %>%
arrange(-beta) %>%
select(topic, term) %>%
summarise(terms = list(term)) %>%
mutate(terms = map(terms, paste, collapse = ", ")) %>%
unnest()
td_beta <- tidy(topic_model)
top_terms <- td_beta %>%
arrange(beta) %>%
group_by(topic) %>%
top_n(7, beta) %>%
arrange(-beta) %>%
select(topic, term) %>%
summarise(terms = list(term)) %>%
mutate(terms = map(terms, paste, collapse = ", ")) %>%
unnest()
library(ggthemes)
top_terms <- td_beta %>%
arrange(beta) %>%
group_by(topic) %>%
top_n(7, beta) %>%
arrange(-beta) %>%
select(topic, term) %>%
summarise(terms = list(term)) %>%
mutate(terms = map(terms, paste, collapse = ", ")) %>%
unnest()
gamma_terms <- td_gamma %>%
group_by(topic) %>%
summarise(gamma = mean(gamma)) %>%
arrange(desc(gamma)) %>%
left_join(top_terms, by = "topic") %>%
mutate(topic = paste0("Topic ", topic),
topic = reorder(topic, gamma))
gamma_terms %>%
ggplot(aes(topic, gamma, label = terms, fill = topic)) +
geom_col(show.legend = FALSE) +
geom_text(hjust=0.6,size = 3,nudge_y = 0.15,
family = "IBMPlexSans") +
coord_flip() +
theme_tufte(base_family = "IBMPlexSans", ticks = FALSE) +
theme(plot.title = element_text(size = 16,
family="IBMPlexSans-Bold"),
plot.subtitle = element_text(size = 13)) +
labs(x = NULL, y = expression(gamma),
title = "6 topics by prevalence in the Speech Corpus",
subtitle = "With the top words that contribute to each topic")
#stemming
speechcorp <- tokens(corpus, remove_punct = TRUE,
remove_symbols = TRUE, remove_numbers = TRUE) %>%
tokens_remove(stop) %>%
tokens_wordstem() %>%
tokens_ngrams(c(1,2))%>%
dfm() %>%
dfm_tolower()%>%
dfm_trim(min_termfreq = 5, min_docfreq = 0.0025, docfreq_type = "prop")
dfm_tfidf()
#stemming
speechcorp <- tokens(corpus, remove_punct = TRUE,
remove_symbols = TRUE, remove_numbers = TRUE) %>%
tokens_remove(stop) %>%
tokens_wordstem() %>%
tokens_ngrams(c(1,2))%>%
dfm() %>%
dfm_tolower()%>%
dfm_trim(min_termfreq = 5, min_docfreq = 0.0025, docfreq_type = "prop")%>%
dfm_tfidf()
speechcorp
textplot_wordcloud(speechcorp, random_order = FALSE, rotation = 0.25,
color = RColorBrewer::brewer.pal(12, "Dark2"),max_words =150,max_size = 4)
library(stm)
library(furrr)
many_models <- data_frame(K = c(2,3,4,5,6,7,8,9)) %>%
mutate(topic_model = future_map(K, ~stm(speechcorp, K = .,
verbose = FALSE)))
speechcorp<-dfm(speechcorp)
library(stm)
library(furrr)
many_models <- data_frame(K = c(2,3,4,5,6,7,8,9)) %>%
mutate(topic_model = future_map(K, ~stm(speechcorp, K = .,
verbose = FALSE)))
model<-stm(speechcorp, prevalence = ~dates,  K=7, seed=123)
?speechcorpo
?convert
speechcorp <- convert(speechcorp, to = "stm")
speechcorp <- convert(speechcorp, to = "lda")
speechcorp <- convert(speechcorp, to = "lda")
speechcorp <- convert(speechcorp, to = "lda")
speechcorp <- convert(speechcorp, to = "data.frame")
covidDfmlda <- convert(speechcorp, to = "lda")
View(speechcorp)
#stemming
speechcorp <- tokens(corpus, remove_punct = TRUE,
remove_symbols = TRUE, remove_numbers = TRUE) %>%
tokens_remove(stop) %>%
tokens_wordstem() %>%
tokens_ngrams(c(1,2))%>%
dfm() %>%
dfm_tolower()%>%
dfm_trim(min_termfreq = 5, min_docfreq = 0.0025, docfreq_type = "prop")%>%
dfm_tfidf()
speechcorp
textplot_wordcloud(speechcorp, random_order = FALSE, rotation = 0.25,
color = RColorBrewer::brewer.pal(12, "Dark2"),max_words =150,max_size = 4)
setwd('/home/leon/Documents/GitHub/stat/Stats/Data')
library(readr)
library(tidyverse)
library(lubridate)
library(tidyr)
dat1<-read_csv('grocery.csv',col_names=FALSE, col_select=-1)
setwd("/home/leon/Documents/Statts/speeches")
prob<-read_csv('prob.csv')
setwd('/home/leon/Documents/GitHub/stat/Stats/Data')
library(readr)
library(tidyverse)
library(lubridate)
library(tidyr)
dat1<-read_csv('grocery.csv',col_names=FALSE, col_select=-1)
prob<-read_csv('prob.csv')
prob<-prob[3:33,]
data<-dat1[,-1]
data<-t(data)
data<-as.data.frame(data)
#data<-data[2:602,1:51]
data$V1<-as.Date(data$V1)
attach(data)
matrix=as.data.frame(matrix(, nrow = 0, ncol = 51))
dates=c('28-01-2020','12-09-2020','20-06-2020','18-08-2020','19-09-2020','10-09-2020','28-08-2020','14-01-2020','13-03-2020','08-08-2020','28-02-2020','10-02-2020','15-04-2020','03-09-2020','19-02-2020','30-01-2020','01-06-2020','01-09-2020','20-02-2020','07-09-2020','11-03-2020','13-09-2020','17-09-2020','04-07-2020','08-09-2020','02-03-2020','23-04-2020','22-09-2020','18-09-2020','13-04-2020','21-02-2020','21-09-2020','05-11-2020')
for (i in seq(1:33)) {
for (date in dates[i]) {
new= data[data$V1 > dmy(date) & data$V1 <= dmy(date)+days(3),]
matrix=rbind(matrix, new)
} }
realmatrix<-lapply(matrix[-1], as.numeric)
realmatrix<-as.data.frame(realmatrix)
n <- 3;
aggregate_groceries<-aggregate(realmatrix, list(rep(1:(nrow(matrix) %/% n + 1), each = n, len = nrow(matrix))), mean)[-1]
prob<-prob[-c(1:4),c(3,6)]
realdf<-c(prob, realmatrix)
realdf<-as.data.frame(realdf)
setwd('/home/leon/Documents/GitHub/stat/Stats/Data')
library(readr)
library(tidyverse)
library(lubridate)
library(tidyr)
dat1<-read_csv('grocery.csv',col_names=FALSE, col_select=-1)
prob<-read_csv('prob.csv')
prob<-prob[-c(1:4)]
realdf<-c(prob, realmatrix)
realdf<-as.data.frame(realdf)
prob<-prob[-c(1:4)]
prob<-prob[-c(1:4),]
realdf<-c(prob, realmatrix)
setwd('/home/leon/Documents/GitHub/stat/Stats/Data')
library(readr)
library(tidyverse)
library(lubridate)
library(tidyr)
dat1<-read_csv('grocery.csv',col_names=FALSE, col_select=-1)
prob<-read_csv('prob.csv')
prob<-prob[-c(1:4),]
realdf<-c(prob, realmatrix)
realdf<-as.data.frame(realdf)
statenames<-dat1[["X2"]]
statenames <- statenames[-1]
#statenames<-as.character(statenames)
statenames<-as.character(statenames)
setnames(realdf, old=c(colnames(realdf[-c(1,2)])),new=c(statenames))
library(data.table)
statenames<-as.character(statenames)
setnames(realdf, old=c(colnames(realdf[-c(1,2)])),new=c(statenames))
library(data.table)
statenames<-as.character(statenames)
setnames(realdf, old=c(colnames(realdf[-c(1,7)])),new=c(statenames))
View(realdf)
library(data.table)
statenames<-as.character(statenames)
setnames(realdf, old=c(colnames(realdf[-c(1,6)])),new=c(statenames))
colnames(realdf[-c(1,6)])
library(data.table)
statenames<-as.character(statenames)
setnames(realdf, old=c(colnames(realdf[-c(1:6)])),new=c(statenames))
newdata<-data.frame(realdf[1:6], stack(realdf[7:53]))
write.csv(newdata, 'stackedgroceries.csv')
data<-read.csv('stackedgroceries.csv')
setwd("/home/leon/Documents/GitHub/stat/Stats/Data")
data<-read.csv('stackedgroceries.csv')
setwd("/home/leon/Documents/GitHub/stat/Stats/Data")
data<-read.csv('stackedgroceries.csv')
View(data)
library(bayestestR)
library(rstanarm)
library(ggplot2)
library(tidyverse)
library(mombf)
View(data)
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(tidyverse)
library(quanteda) # quantitative analysis of textual data  (https://quanteda.io/articles/quickstart.html)
library(quanteda.textplots) # complementary to quanteda, for visualization
library(cld3) # for language detection
library(lda) # implementation of Latent Dirichlet Allocation
library(servr) # will be used for visualization
# library(topicmodels) # alternative to lda, several topic models included
library(stm) # for structural topic modeling
set.seed(5528) # set seed for reproductibility
library(corpus)
setwd("/home/leon/Documents/Statts/speeches")
speeches<-read_csv('output_file.csv',show_col_types = FALSE)
dates=c('28-01-2020','12-09-2020','20-06-2020','18-08-2020','19-09-2020','10-09-2020','28-08-2020','14-01-2020','13-03-2020','08-08-2020','28-02-2020','10-02-2020','15-04-2020','03-09-2020','19-02-2020','30-01-2020','01-06-2020','01-09-2020','20-02-2020','07-09-2020','11-03-2020','13-09-2020','17-09-2020','04-07-2020','08-09-2020','02-03-2020','23-04-2020','22-09-2020','18-09-2020','13-04-2020','21-02-2020','21-09-2020','05-11-2020')
#dates<-str_split_fixed(speeches$FileName,pattern='(.{3}\\d\\d_\\d{4}(?=.txt$))',n=33)
speeches$dates=as.Date(dates,'%d-%m-%y')
corpus <- corpus(speeches, docid_field = 'FileName', text_field = 'Content')
languages <- detect_language(corpus)
table(languages)
numspeechcorp <- ntoken(corpus)
data.frame(numspeechcorp) %>% ggplot(aes(numspeechcorp)) + geom_histogram(binwidth = 45) + xlab('Number of tokens')
speechcorp<-tokens(corpus)
cloudspeechcorp<- dfm(speechcorp)
cloudspeechcorp
textplot_wordcloud(speechcorp, random_order = FALSE, rotation = 0.25,
color = RColorBrewer::brewer.pal(8, "Dark2"))
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(tidyverse)
library(quanteda) # quantitative analysis of textual data  (https://quanteda.io/articles/quickstart.html)
library(quanteda.textplots) # complementary to quanteda, for visualization
library(cld3) # for language detection
library(lda) # implementation of Latent Dirichlet Allocation
library(servr) # will be used for visualization
# library(topicmodels) # alternative to lda, several topic models included
library(stm) # for structural topic modeling
set.seed(5528) # set seed for reproductibility
library(corpus)
library(tidytext)
book_words <- speeches %>%
unnest_tokens(word, Content) %>%
count(FileName, word, sort = TRUE)
book_words <- book_words%>%
filter(!word %in% stop)
stop<-c(stopwords("en"), 'go','said','people','know','right','like', 'year', 'get', 'one','never', 'ever', 'much', 'back','good','right','say','now','done','want','even','see','thank','great', 'guy','just','think','way','new','us','got','going','many','well','can','really','make','job','lot','take','come','tell','q','yeah','thing','look','happen','okay','something','look','mnuchin','birx','happen','keep','things','happens','happening','talks,','talking','getting','letting','opening', 'coming','president','presidential','beieve','believing','ahead', 'please','talk', 'agree','continue', 'specify', 'specific','use', 'using','also', 'large', 'show', 'thousand','time','love','mr','sir', 'america', 'american', 'year', 'countries', 'country', 'state','year','years','win','we\'re','it\'s','they\'re','we’re','it’s','they’re','that’s', 'don’t', 'i’ve','i’m', 'he’s','we’ve', 'will','didn’t','we’ll', 'dr', 'you’re')
book_words <- book_words%>%
filter(!word %in% stop)
book_words <- book_words %>%
anti_join(stop_words)
total_words <- book_words %>%
group_by(FileName) %>%
summarize(total = sum(n))
book_words <- left_join(book_words, total_words)
book_tf_idf <- book_words %>%
bind_tf_idf(word, FileName, n)
final<-book_tf_idf[order(book_tf_idf$tf, decreasing = TRUE),]
library(tm)
tdm <- cast_dtm(data=book_tf_idf,term=word, document=FileName, n)
ap_lda <- LDA(tdm, k = 4, control = list(seed = 1234))
library(topicmodels)
ap_lda <- LDA(tdm, k = 4, control = list(seed = 1234))
ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics
gammaDF <- as.data.frame(ap_lda@gamma)
names(gammaDF) <- c(1:4)
View(gammaDF)
gammaDF <- as.data.frame(ap_lda@gamma)
names(gammaDF) <- c(1:4)
write.csv(gammaDF,'prob2.csv')
setwd('/home/leon/Documents/GitHub/stat/Stats/Data')
library(readr)
library(tidyverse)
library(lubridate)
library(tidyr)
dat1<-read_csv('grocery.csv',col_names=FALSE, col_select=-1)
prob<-read_csv('prob2.csv')
setwd('/home/leon/Documents/GitHub/stat/Stats/Data')
library(readr)
library(tidyverse)
library(lubridate)
library(tidyr)
dat1<-read_csv('grocery.csv',col_names=FALSE, col_select=-1)
prob<-read_csv('prob2.csv')
data<-dat1[,-1]
data<-t(data)
data<-as.data.frame(data)
#data<-data[2:602,1:51]
data$V1<-as.Date(data$V1)
attach(data)
matrix=as.data.frame(matrix(, nrow = 0, ncol = 51))
dates=c('28-01-2020','12-09-2020','20-06-2020','18-08-2020','19-09-2020','10-09-2020','28-08-2020','14-01-2020','13-03-2020','08-08-2020','28-02-2020','10-02-2020','15-04-2020','03-09-2020','19-02-2020','30-01-2020','01-06-2020','01-09-2020','20-02-2020','07-09-2020','11-03-2020','13-09-2020','17-09-2020','04-07-2020','08-09-2020','02-03-2020','23-04-2020','22-09-2020','18-09-2020','13-04-2020','21-02-2020','21-09-2020','05-11-2020')
for (i in seq(1:33)) {
for (date in dates[i]) {
new= data[data$V1 > dmy(date) & data$V1 <= dmy(date)+days(3),]
matrix=rbind(matrix, new)
} }
realmatrix<-lapply(matrix[-1], as.numeric)
realmatrix<-as.data.frame(realmatrix)
prob<-prob[-c(1:4),]
realdf<-c(prob, realmatrix)
realdf<-as.data.frame(realdf)
statenames<-dat1[["X2"]]
statenames <- statenames[-1]
#statenames<-as.character(statenames)
View(realdf)
n <- 3;
aggregate_groceries<-aggregate(realmatrix, list(rep(1:(nrow(matrix) %/% n + 1), each = n, len = nrow(matrix))), mean)[-1]
View(aggregate_groceries)
prob<-prob[-c(1:4),]
realdf<-c(prob, aggregate_groceries)
realdf<-as.data.frame(realdf)
setwd('/home/leon/Documents/GitHub/stat/Stats/Data')
library(readr)
library(tidyverse)
library(lubridate)
library(tidyr)
dat1<-read_csv('grocery.csv',col_names=FALSE, col_select=-1)
prob<-read_csv('prob2.csv')
prob<-prob[-c(1:4),]
realdf<-c(prob, aggregate_groceries)
realdf<-as.data.frame(realdf)
statenames<-dat1[["X2"]]
statenames <- statenames[-1]
#statenames<-as.character(statenames)
View(realdf)
library(data.table)
statenames<-as.character(statenames)
setnames(realdf, old=c(colnames(realdf[-c(1:5)])),new=c(statenames))
newdata<-data.frame(realdf[1:5], stack(realdf[7:53]))
newdata<-data.frame(realdf[1:5], stack(realdf[7:53]))
write.csv(newdata, 'stackedgroceries.csv')
View(newdata)
setwd("/home/leon/Documents/GitHub/stat/Stats/Data")
data<-read.csv('stackedgroceries.csv')
View(data)
newdata<-data.frame(realdf[1:5], stack(realdf[7:53]))
write.csv(newdata, 'stackedgroceries.csv')
setwd("/home/leon/Documents/GitHub/stat/Stats/Data")
data<-read.csv('stackedgroceries.csv')
View(data)
X= model.matrix(~ X1+X2+X3+X4, data=data)
y= data$values
n= nrow(X)
gseq= exp(seq(log(.01),log(1),length=20))
gseq
library(mvtnorm)
V= diag(ncol(X))
beta= rmvnorm(1000, sigma= V)
sse= colSums((X %*% t(beta))^2) / n
r2= double(length(gseq))
for (i in 1:length(gseq)) {
r2[i]= mean(sse * gseq[i] / (1 + sse * gseq[i]))
}
par(mar=c(4,5,.1,.1), cex.lab=1.3, cex.axis=1.3)
plot(gseq, r2, type='l', xlab='g', ylab=expression(paste('Theoretical ',R^2)))
plot(gseq, r2, type='o', xlab='g', ylab=expression(paste('Theoretical ',R^2)))
fit.mle= lm(y ~ X[,-1]) #1st column in x is the intercept, already added by lm
b.mle= coef(fit.mle)
summary(fit.mle)
X= model.matrix(~ X2+X3+X4, data=data)
y= data$values
n= nrow(X)
gseq= exp(seq(log(.01),log(1),length=20))
gseq
#prior elicitation
library(mvtnorm)
V= diag(ncol(X))
beta= rmvnorm(1000, sigma= V)
sse= colSums((X %*% t(beta))^2) / n
r2= double(length(gseq))
for (i in 1:length(gseq)) {
r2[i]= mean(sse * gseq[i] / (1 + sse * gseq[i]))
}
par(mar=c(4,5,.1,.1), cex.lab=1.3, cex.axis=1.3)
plot(gseq, r2, type='o', xlab='g', ylab=expression(paste('Theoretical ',R^2)))
#comparison with MLE
fit.mle= lm(y ~ X[,-1]) #1st column in x is the intercept, already added by lm
b.mle= coef(fit.mle)
summary(fit.mle)
fit.bayes <- stan_glm(y ~ X[,-1], family = gaussian(link = "identity"), algorithm='sampling', refresh=0)
b.bayes= coef(fit.bayes)
b.bayes
data.frame(mle= b.mle[-1], bayes= b.bayes[-1]) %>%
ggplot(aes(x=mle,y=bayes)) +
geom_point(shape = "O",size=2) +
geom_abline(slope=1, intercept = 0, linetype = 'dashed') +
geom_hline(yintercept = 0, linetype = 'dotted') +
xlab('MLE OLS') +
ylab('MCMC Bayesian regression') +
coord_cartesian(xlim=c(-2,0.5),ylim=c(-2,0.5)) +
theme_classic()
data.frame(mle= b.mle[-1], bayes= b.bayes[-1]) %>%
ggplot(aes(x=mle,y=bayes)) +
geom_point(shape = "O",size=2) +
geom_abline(slope=1, intercept = 0, linetype = 'dashed') +
geom_hline(yintercept = 0, linetype = 'dotted') +
xlab('MLE OLS') +
ylab('MCMC Bayesian regression') +
coord_cartesian(xlim=c(1,10),ylim=c(1,10)) +
theme_classic()
fit.bayesreg <- modelSelection(y=y,x=X, priorCoef=zellnerprior(taustd=1), priorDelta=modelbbprior(1,1))
head(postProb(fit.bayesreg),10)
ci.bayesreg <- coef(fit.bayesreg)[-c(1,nrow(coef(fit.bayesreg))),]
sel.bayesreg <- ci.bayesreg[,4] > 0.5
ci.bayesreg[,1:3]= round(ci.bayesreg[,1:3], 3)
ci.bayesreg[,4]= round(ci.bayesreg[,4], 4)
head(ci.bayesreg)
plot(NA, ylim=1.25*range(ci.bayesreg[,1:3]), xlim=c(0,nrow(ci.bayesreg)), ylab='95% CI', xlab='', main='Bayesian Model Selection')
cols= ifelse(beta < ci.bayesreg[ , 1] | beta > ci.bayesreg[, 2], 2, 1)
segments(y0 = ci.bayesreg[, 2], y1 = ci.bayesreg[, 3], x0 = 1:nrow(ci.bayesreg), col = cols)
points(1:p, beta, pch = 16)
plot(NA, ylim=1.25*range(ci.bayesreg[,1:3]), xlim=c(0,nrow(ci.bayesreg)), ylab='95% CI', xlab='', main='Bayesian Model Selection')
cols= ifelse(beta < ci.bayesreg[ , 1] | beta > ci.bayesreg[, 2], 2, 1)
segments(y0 = ci.bayesreg[, 2], y1 = ci.bayesreg[, 3], x0 = 1:nrow(ci.bayesreg), col = cols)
points(1:3, beta, pch = 16)
plot(NA, ylim=1.25*range(ci.bayesreg[,1:3]), xlim=c(0,nrow(ci.bayesreg)), ylab='95% CI', xlab='', main='Bayesian Model Selection')
cols= ifelse(beta < ci.bayesreg[ , 1] | beta > ci.bayesreg[, 2], 2, 1)
segments(y0 = ci.bayesreg[, 2], y1 = ci.bayesreg[, 3], x0 = 1:nrow(ci.bayesreg), col = cols)
points(1:4, beta, pch = 16)
plot(NA, ylim=1.25*range(ci.bayesreg[,1:3]), xlim=c(0,nrow(ci.bayesreg)), ylab='95% CI', xlab='', main='Bayesian Model Selection')
cols= ifelse(beta < ci.bayesreg[ , 1] | beta > ci.bayesreg[, 2], 2, 1)
segments(y0 = ci.bayesreg[, 2], y1 = ci.bayesreg[, 3], x0 = 1:nrow(ci.bayesreg), col = cols)
points(1:4, beta, pch = 16)
gammaDF <- as.data.frame(ap_lda@gamma)
names(gammaDF) <- c(1:4)
prob<-cbind(gammaDF, dates)
write.csv(prob,'prob2.csv')
View(prob)
View(book_tf_idf)
View(ap_lda)
ap_lda@terms
View(speechcorp)
View(ap_topics)
ap_topics<-filter(ap_topics, ap_topics$topics==3)
?filter
ap_topics<-filter(ap_topics, ap_topics$topic==3)
View(ap_topics)
ap_topics<-filter(ap_topics, ap_topics$topic==3)[order(ap_topics$beta),]
View(ap_topics)
library(tm)
tdm <- cast_dtm(data=book_tf_idf,term=word, document=FileName, n)
library(topicmodels)
ap_lda <- LDA(tdm, k = 4, control = list(seed = 1234))
ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics
ap_topics<-filter(ap_topics, ap_topics$topic==3)[order(ap_topics$beta),]
View(ap_topics)
View(ap_topics)
View(ap_topics)
View(ap_topics)
book_tf_idf <- book_words %>%
bind_tf_idf(word, FileName, n)
final<-book_tf_idf[order(book_tf_idf$tf, decreasing = TRUE),]
library(tm)
tdm <- cast_dtm(data=book_tf_idf,term=word, document=FileName, n)
library(topicmodels)
ap_lda <- LDA(tdm, k = 4, control = list(seed = 1234))
ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics
View(ap_topics)
ap_topics<-filter(ap_topics, ap_topics$topic==3)[order(ap_topics$beta),]
View(ap_topics)
library(topicmodels)
ap_lda <- LDA(tdm, k = 4, control = list(seed = 1234))
ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics
covidwords<-filter(ap_topics, ap_topics$topic==3)[order(ap_topics$beta),]
View(covidwords)
covidwords<-filter(ap_topics, ap_topics$topic==3)
View(covidwords)
ap_top_terms <- ap_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
ap_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered()
covidwords<-filter(ap_topics, ap_topics$topic==3)
covidwords<covidwords[order(ap_topics$beta),]
covidwords<-filter(ap_topics, ap_topics$topic==3)
covidwords<covidwords[order(covidwords$beta),]
View(covidwords)
covidwords<-filter(ap_topics, ap_topics$topic==3)
covidwords<covidwords[order(beta),]
covidwords<-filter(ap_topics, ap_topics$topic==3)
covidwords$beta<-as.numeric(covidwords$beta)
covidwords<covidwords[order(covidwords$beta),]
View(covidwords)
class(covidwords$beta)
View(covidwords)
View(covidwords)
covidwords<-filter(ap_topics, ap_topics$topic==3)
covidwords$beta<-as.numeric(covidwords$beta)
covidwords<covidwords[order(covidwords$beta),]
write.csv(covidwords, 'covidwords.csv')
